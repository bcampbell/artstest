#!/usr/bin/env python
""" compare

Compare output from a news article scraper against expected data.

The .expected data files are designed to be easy to edit by hand.
They begin with a yaml front-matter section, then the rest
of the file is the text content that we expect to be extracted.
For now, we expect the scraper to produce that same format, ready to
be passed into this program.
"""


from __future__ import print_function
import re
import sys
from pprint import pprint
import datetime
import argparse     # >=python 2.7
import unicodedata

import yaml
import lxml.html
import lxml.etree
import distance

splitter = re.compile(r'---(.*?)---\s*(.*)$',re.DOTALL)

ALL_CHECKS = ['headline','urls','dates','content','authors']

OPTS = None

def main():
    global OPTS
    parser = argparse.ArgumentParser()
    parser.add_argument('got', help='file containing scraped data')
    parser.add_argument('expected', help='file containing expected data')


    parser.add_argument('-d', '--debug', action="store_true", help="dump out data to aid debugging")
    parser.add_argument('-c', '--check',
        action='append',
        help="only check specified fields",
        choices=ALL_CHECKS)
#    parser.add_argument("-v", "--verbose", help="increase output verbosity",
#                    action="store_true")
    OPTS = parser.parse_args()


    try:
        # load and compare two articles
        art_got = Article(OPTS.got)
        art_expected = Article(OPTS.expected)


        checks = ALL_CHECKS
        if OPTS.check:
            checks = OPTS.check

        errs = art_got.compare(art_expected, checks)
        if len(errs)>0:
            [print("  %s" % (err.encode('utf-8'),), file=sys.stdout) for err in errs]

            return 1
        else:
            return 0

    except BaseException as e:
        print("error: %s" % (e,), file=sys.stderr)
        raise
        return 2


class Article:
    def __init__(self, filename): 
        self.filename=filename
        f = open(filename,"r")
        with f:
            data = f.read()
            # split yaml front matter from content block
            m = splitter.match(data)
            assert(m)
            meta = yaml.load(m.group(1))
            for f in ("headline","published","updated","authors","canonical_url","urls"):
                if f in meta:
                    self.__dict__[f] = meta[f]
                else:
                    if f in ("authors","urls"):
                        self.__dict__[f] = []
                    else:
                        self.__dict__[f] = None

            # some patchups
            if self.urls is None:
                self.urls = []

            self.content = unicode(m.group(2), "utf-8")

    def dump(self):
        print("canonical_url: %s" % (self.canonical_url))
        print("headline: %s" % (self.headline))
        print("published: %s" % (self.published))
        print("updated: %s" % (self.updated))
        print("authors: %s" % (self.authors))
        print("------")
        print(self.content)

    def compare(self,expected, checks=ALL_CHECKS):

        errs = []

        # check urls
        if 'urls' in checks:
            if self.canonical_url != expected.canonical_url:
                errs.append("canonical_url: expected '%s', got '%s'" %(expected.canonical_url,self.canonical_url))
            if sorted(self.urls) != sorted(expected.urls):
                errs.append("urls: expected '%s', got '%s'" %(expected.urls,self.urls))

        # check headline
        if 'headline' in checks:
            if self.headline != expected.headline:
                errs.append("Headline: expected '%s', got '%s'" %(expected.headline,self.headline))

        # check authors
        if 'authors' in checks:
            for author in expected.authors:
                if self.find_author(author) is None:
                    errs.append("missing author: '%s'" % (author['name'],))
            for author in self.authors:
                if expected.find_author(author) is None:
                    errs.append("extra author: '%s'" % (author['name'],))

        # check content
        if 'content' in checks:
            txt1 = html_to_text(self.content)
            txt2 = html_to_text(expected.content)

            txt1 = normalise_text(txt1)
            txt2 = normalise_text(txt2)

            if OPTS.debug:
                print("-"*20+"got"+"-"*20)
                print(txt1)
                print("-"*20+" expected "+"-"*20)
                print(txt2)
                print("-"*80)

            if txt1 != txt2:
                # allow a little deviation...

                bi1 = bigrams(txt1)
                bi2 = bigrams(txt2)

                # filter out bigrams containing whitespace
                bi1 = [ bi for bi in bi1 if bi[0]!= ' ' and bi[1] != ' ' ]
                bi2 = [ bi for bi in bi2 if bi[0]!= ' ' and bi[1] != ' ' ]
                s1 = set(bi1)
                s2 = set(bi2)

                # dice coeffcient (0=similar, 1=dissimilar)
                dice = 1.0 - ( 2.0*float(len(s1.intersection(s2))) ) / float(len(s1) + len(s2))

                # if got is a subset of expected, then we're a lot more forgiving.
                # It's OK to scrape a bit of extra cruft as long as we've got the text covered...
                if s2.issubset(s1):
                    if dice >= 0.05:
                        errs.append("content mismatch - extra cruft (dice coefficient %.4f)" % (dice,))
                else:
                    if dice >= 0.0001:
                        errs.append("content mismatch - missing text (dice coefficient %.4f)" % (dice,))




                #d = distance.jaccard(txt1.split(), txt2.split())
                #if d >= 0.05:
                #    errs.append("content mismatch (jaccard dist: %.3f)" % (d,))

                #d = dicenum(txt1,txt2)
                #if d >= 0.005:
                #    errs.append("content mismatch (dice index: %.3f)" % (d,))



                # is the expected stuff there?
#                parts = txt1.partition(txt2)
#                if len(parts[2])>0:
                    # yes, expected result is contained
#                    errs.append("content mismatch - %.1f%% cruft (%d before, %d after)" % ((len(parts[0])+len(parts[2]))/float(len(txt1)), len(parts[0]), len(parts[2])))
#                else:
#                    errs.append("content mismatch")


        # check dates
        if 'dates' in checks:
            pub_err = test_dates(self.published, expected.published)
            if pub_err is not None:
                errs.append("published: " + pub_err)


        # TODO: anything else :-)

        return errs

    def find_author(self,author):
        for a in self.authors:
            if a['name'] == author['name']:
                return a
        return None



def test_dates(got,expect):
    """ compare dates. Any time component is ignored. """
    if expect is None and got is not None:
        return "expected none, got %s" % (got)
    if expect is not None and got is None:
        return "expected %s, got none" % (expect)


    # silly yaml. dates could be strings, dates or datetimes.
    expect_date = fudge_date(expect)
    got_date = fudge_date(got)

    if expect_date != got_date:
        return "expected %s, got %s" % (expect,got)
    return None


datepat = re.compile(r'\d{4}-\d{2}-\d{2}')

def fudge_date(d):
    """ convert whatever into a yyyy-mm-dd string """

    if d is None:
        return None

    if type(d) == datetime.date:
        return d.isoformat()

    if type(d) == datetime.datetime:
        return d.date().isoformat()

    # assume it's a string
    m = datepat.search(d)
    if m is None:
        return None
    return m.group(0)


def normalise_text(txt):
    """ return stripped-down, ascii, alphanumeric-only version for comparisons """
    # replace various accented latin chars with rough ascii equivalents
    txt = unicodedata.normalize('NFKD',txt).encode('ascii','ignore')
    txt = re.sub(ur'[^a-zA-Z0-9\s]',u'',txt)
    txt = u" ".join(re.split(ur'\s+',txt))    # compress spaces
    txt = txt.lower().strip()
    return txt


def uberstrip(s):
    # strip leading/trailing non-alphabetic chars
    pat = re.compile(r'^[^\w()]*(.*?)[^\w()]*$', re.IGNORECASE|re.UNICODE)
    return pat.sub(r'\1', s)


def html_to_text(html):
    if html.strip() == "":
        return u""
    parser = lxml.html.HTMLParser()
    doc = lxml.html.document_fromstring(html, parser)
    return render_text(doc)




def render_text(el):
    """ like lxml.html text_content(), but with tactical use of whitespace for block elements """

    inline_tags = ( 'a', 'abbr', 'acronym', 'b', 'basefont', 'bdo', 'big',
        'br',
        'cite', 'code', 'dfn', 'em', 'font', 'i', 'img', 'input',
        'kbd', 'label', 'q', 's', 'samp', 'select', 'small', 'span',
        'strike', 'strong', 'sub', 'sup', 'textarea', 'tt', 'u', 'var',
        'applet', 'button', 'del', 'iframe', 'ins', 'map', 'object',
        'script' )

    txt = u''

    tag = str(el.tag).lower()
    if tag not in inline_tags:
        txt += u"\n";

    if el.text is not None:
        txt += unicode(el.text)
    for child in el.iterchildren():
        txt += render_text(child)
        if child.tail is not None:
            txt += unicode(child.tail)

    if el.tag=='br' or tag not in inline_tags:
        txt += u"\n";
    return txt



def bigrams(txt):
    """ split a string into bigrams """
    out = []
    if len(txt) <2:
        return[]

    prev = txt[0]
    for ch in txt[1:]:
        out.append( prev + ch )
        prev=ch

    return out


def dicenum(txt1 , txt2):
    """ returns Sorensen-Dice index in range 0 (similar) to 1 (dissimilar) """

    out1 = bigrams(txt1)
    out2 = bigrams(txt2)
    # filter out bigrams containing whitespace
    out1 = [ bi for bi in out1 if bi[0]!= ' ' and bi[1] != ' ' ]
    out2 = [ bi for bi in out2 if bi[0]!= ' ' and bi[1] != ' ' ]
    s1 = set(out1)
    s2 = set(out2)

    return 1.0 - ( 2.0*float(len(s1.intersection(s2))) ) / float(len(s1) + len(s2))




if __name__ == "__main__":
    sys.exit(main())

